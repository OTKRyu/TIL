# 3 신경망

## 3-1 퍼셉트론에서 신경망으로

- 신경망은 입력층과 출력층 그리고 그 사이에 계산을 수행하는 은닉층으로 이루어져 있다.(출력층도 계산을 한다.)
- 활성화 함수란 입력값과 편향치를 받아 활성화할 것인지 아닌지를 0과 1로 나타내주는 함수다.

## 3-2 활성화 함수

기본적으로 일정 경계점으로 숫자가 달라지는 함수를 계단함수라고 부르는데 활성화함수들은 대체로 이런 계단함수처럼 되어있다.

1. 시그모이드 함수

2. 계단 함수

   1. 둘 간의 비교

      시그모이드 함수는 매끄러운 연속함수기 때문에 학습을 단번에 진행시키는 것이 아닌 천천히 지속적으로 학습이 일어나게 만들어준다.

      이 두 함수를 보면 선형 함수가 아님을 알 수 있는데 선형 함수의 경우 어떠한 연산을 거듭한다고 해도 선형함수의 형태를 벗어나기 어렵기 때문에 원하는 형태를 만들기 위해서는 비선형 함수의 존재가 중요하다.

3. ReLU 함수

   입력이 0을 넘으면 그 입력을 그대로 출력, 아니면 0을 출력

## 3-3 다차원 배열의 계산

일반적으로 알고 있는 행렬의 계산을 numpy의 array기능을 이용해 수행할 수 잇다.

- `dot` : 행렬의 곱셈에 해당하는 내적을 수행한다.

이를 통해 신경망 1층에 해당하는 연산을 한번에 처리할 수 있다.

그 후 데이터가 모두 전달된 뉴런에 활성화함수를 적용하여 활성화정도를 결정한다.

이 때 출력층에서는 굳이 아무것도 안하고 전달만 하고 끝내도 좋지만, 통일성을 위해 활성화함수를 쓰는데 이 때 identity function을 쓴다.

기계 학습의 목적은 대체로 분류와 회귀로 나뉘는데 분류는 데이터가 어느 클래스에 속하는 지를 의미하며 대체로 항등함수를 쓰며, 회귀는 연속적인 수치를 예측하는 문제로 소프트맥스 함수를 사용한다.

### 소프트맥스 함수
- 소프트맥스 함수는 모든 입력치를 오일러상수로 지수화한 후 이 중 원소가 얼마의 비중을 차지하는 지를 되돌려주는 함수다.
- 소프트맥스 함수 구현시 지수화의 문제점인 floating point 문제가 발생하기 쉬우니 조심해야한다. 이를 해결하는 방법으로 최댓값을 빼서 지수화당하는 수들을 작게 만듬으로서 해결하는 방법이 있다.(overflow방지도 된다.)
- 특징으로 전체중 차지하는 비중을 반환하는 함수이기때문에 확률과 같은 성질을 가진다.
- 다만 지수계산이 주를 이루는 함수이다보니 학습단계의 출력층에서는 계산을 덜 하기 위해 생략하는 편이다.

## mnist 관련
직접 읽어보자 자세한 코드를 엿볼 수 있다.
정규화나 백색화(데이터 균일화)와 같은 전처리를 통해 효율을 상승시킬 수 있다.
학습은 배치 단위로 일어나게 되는데 이를 잘 처리하면 한번에 많은 계산을 행렬로 처리할 수 있다.(작은 병렬 계산 여러번 보다 큰 계산 한번을 컴퓨터가 더 잘 수행하기 때문)


