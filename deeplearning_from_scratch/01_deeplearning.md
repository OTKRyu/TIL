# 3 신경망

## 3-1 퍼셉트론에서 신경망으로

- 신경망은 입력층과 출력층 그리고 그 사이에 계산을 수행하는 은닉층으로 이루어져 있다.(출력층도 계산을 한다.)
- 활성화 함수란 입력값과 편향치를 받아 활성화할 것인지 아닌지를 0과 1로 나타내주는 함수다.

## 3-2 활성화 함수

기본적으로 일정 경계점으로 숫자가 달라지는 함수를 계단함수라고 부르는데 활성화함수들은 대체로 이런 계단함수처럼 되어있다.

1. 시그모이드 함수

2. 계단 함수

   1. 둘 간의 비교

      시그모이드 함수는 매끄러운 연속함수기 때문에 학습을 단번에 진행시키는 것이 아닌 천천히 지속적으로 학습이 일어나게 만들어준다.

      이 두 함수를 보면 선형 함수가 아님을 알 수 있는데 선형 함수의 경우 어떠한 연산을 거듭한다고 해도 선형함수의 형태를 벗어나기 어렵기 때문에 원하는 형태를 만들기 위해서는 비선형 함수의 존재가 중요하다.

3. ReLU 함수

   입력이 0을 넘으면 그 입력을 그대로 출력, 아니면 0을 출력

## 3-3 다차원 배열의 계산

일반적으로 알고 있는 행렬의 계산을 numpy의 array기능을 이용해 수행할 수 잇다.

- `dot` : 행렬의 곱셈에 해당하는 내적을 수행한다.

이를 통해 신경망 1층에 해당하는 연산을 한번에 처리할 수 있다.

그 후 데이터가 모두 전달된 뉴런에 활성화함수를 적용하여 활성화정도를 결정한다.

이 때 출력층에서는 굳이 아무것도 안하고 전달만 하고 끝내도 좋지만, 통일성을 위해 활성화함수를 쓰는데 이 때 identity function을 쓴다.

기계 학습의 목적은 대체로 분류와 회귀로 나뉘는데 분류는 데이터가 어느 클래스에 속하는 지를 의미하며 대체로 항등함수를 쓰며, 회귀는 연속적인 수치를 예측하는 문제로 소프트맥스 함수를 사용한다.

### 소프트맥스 함수
- 소프트맥스 함수는 모든 입력치를 오일러상수로 지수화한 후 이 중 원소가 얼마의 비중을 차지하는 지를 되돌려주는 함수다.
- 소프트맥스 함수 구현시 지수화의 문제점인 floating point 문제가 발생하기 쉬우니 조심해야한다. 이를 해결하는 방법으로 최댓값을 빼서 지수화당하는 수들을 작게 만듬으로서 해결하는 방법이 있다.(overflow방지도 된다.)
- 특징으로 전체중 차지하는 비중을 반환하는 함수이기때문에 확률과 같은 성질을 가진다.
- 다만 지수계산이 주를 이루는 함수이다보니 학습단계의 출력층에서는 계산을 덜 하기 위해 생략하는 편이다.

## mnist 관련
직접 읽어보자 자세한 코드를 엿볼 수 있다.
정규화나 백색화(데이터 균일화)와 같은 전처리를 통해 효율을 상승시킬 수 있다.
학습은 배치 단위로 일어나게 되는데 이를 잘 처리하면 한번에 많은 계산을 행렬로 처리할 수 있다.(작은 병렬 계산 여러번 보다 큰 계산 한번을 컴퓨터가 더 잘 수행하기 때문)

# 4 신경망 학습
## 4-1 데이터에서 학습
기계 학습을 크게 두 개로 분류하면,
- 사람이 생각한 특징을 기반으로 기계학습
- 신경망학습(데이터주도)
  - 훈련을 위한 데이터와 시험을 위한 데이터를 나누는데, 이 두가지를 같이 쓰면 범용성면에서 안 좋을 수 있기 떄문이다. 이렇게 특정 케이스에만 과도하게 적응된 상태를 overfitting이라고 한다.

## 4-2 손실 함수

학습에 틀린 것과 맞는 것만으로 이루어져 있다면 얼마나 진행했는지를 알 수 없듯이, 틀렸더라도 얼마나 틀렸고 얼마나 맞았는지를 알아야만 이를 반영해 다음을 준비할 수 있다.

이 얼마나 틀렸고 얼마나 맞았는지를 나타내는 함수가 손실 함수이다.

이 손실 함수로 쓰는 것은 크게 두 가지로 평균 제곱 오차과 교차 엔트로피 오차를 사용한다.

- 평균 제곱 오차 : 말 그대로 오차의 제곱들을 모두 더해서 수의 개수로 나눈것이다.
- 교차 엔트로피 오차 : `-tlog(y)`의 합이다(t는 정답일 때는 1 아닐때는 0이고, y는 각 상황별 출력값)
  - 교차 엔트로피는 정답일 때의 확률만을 증폭시키는 함수로서 오차가 적을 때는 더 적게, 클 때는 더 크게 발생시킨다
  - log계산할 때 0에 가까운 값을 넣으면 오류나기 쉬우니 아주 작은 값이라도 추가로 더해줘서 보정하는 편이 에러를 방지할 수 있다.

이 때 모든 데이터를 학습하여 오류를 검증하는 것은 현실적으로 어려우므로 배치 사이즈를 줄여서 검증해보게 되는데 이를 미니배치 학습이라고 부른다.

정확도를 손실 함수로 쓰지 않는 이유는 연속적인 값을 안정적으로 뽑아낼 수 없기 때문이다. 정확도는 기본적으로 맞은 개수를 기반으로 뽑아내기 때문에 어느정도 방향은 나타내 줄 수 있지만, 그 값이 안정적으로 조금씩 진행하는 것과는 거리가 멀고, 그 데이터셋 내에서 다 맞출 수 있게 fitting이 되는 순간 더 이상 학습할 수 없게 된다.

## 4-3 수치 미분

대학 시절을 복습해보기 바랍니다.

미적분학 후반부 내용으로 미분과 수치 해석의 미분 계산법 및 편미분에 관한 내용

## 4-4 기울기

- 경사법(gradient method) : 말 그대로 그래디언트의 값을 이용하여 이동하는 좌표를 결정짓는 방법으로 이 때 그래디언트의 값을 얼마나 반영시킬지를 학습률이라고 한다. 그리고 학습률과 같이 사람이 결국 지정해야하는 수치를 hyper parameter라고 한다.
- 실제 구현도 책에는 있으나 내용이 방대하고 적는다고 할 수 있는게 없으며 책을 참고하는 편이 훨씬 이득이라고 생각해 적지 않는다.

