# 5 오차 역전파법

- 순전파: 계산식을 구성하는 순서대로 받은 값들로부터 출발해 계산결과값으로 나아가는 과정
- 역전파: 계산식을 구성하는 순서의 역순으로, 계산결과값에서 시작해 받았을 값으로 나아가는 과정

- 연쇄법칙: 미적분학 참조

## 실제 역전파

각종 기본 노드들에 대한 연산

- 덧셈: 그대로
- 곱셈: x,y의 곱셈인 경우 서로 반대인 y,x를 곱해서 전파

실제 구현 책 참조

- relu : 0인 부분에서는 후반에 경향끼치는 것이 없음, 그 외일때는 x이므로 미분값 1만큼 영향을 끼침 고로 0일때는 0을 곱해서, x일 때는 1만큼 곱해서 역전파
- sigmoid : 시그모이드는 앞에서부터 -1 곱하기, 지수함수, 더하기 1, 역수 취하기 순으로 합성된 함수로 역으로 이를 따라서 나가면서 역전파를 일으키면 된다. 시그모이드 함수의 역전파를 이 아래의 조합으로 구현해도 되고 그냥 통으로 구현해도 된다.
  - 역수 취하기는 -y^2배를 해서 역전파
  - 덧셈은 앞에서 봤듯이 그대로 역전파
  - 지수함수는 지수함수는 미분해도 자기자신과 미분값으로 이루어지므로 exp(-x)를 추가로 곱해서 역전파
  - -1 곱하기는 곱셈이므로 -1를 곱해서 역전파

## affine/softmax계층 구현

### affine 계층

아핀 계층은 순전파 때 수행하는 행렬의 내적을 기하학에서 어파인 변환이라고 부르기 때문에 이렇게 이름 붙였다.

위에서는 단일 노드를 대상으로 했지만 실제 기계학습은 행렬단위로 일어나게 된다.

이것을 행렬로 나타내면 XdotW+B = Y가 되는 식인데 dot은 dot product고 B는 편향 행렬, X는 입력값행렬, W는 가중치행렬이다.

이제 이 행렬식을 역전파 시켜야한다. 이를 앞에서 한것처럼 일일히 증명할 수 있으나 이 책에서는 생략했다.

행렬이라고 하더라도 덧셈의 경우 행렬의 모양의 변화도 없고 그대로 더해지므로 오차의 편미분도 그대로 전파가 되고 편향에도 똑같이 전파된다. 다만 dot product의 경우 단일 노드의 경우와 좀 다르게 가중치 행렬의 전치행렬만큼 곱해서 입력값행렬쪽으로, 입력값행렬의 전치행렬만큼 곱해서 가중치행렬쪽으로 역전파하게 된다.